\chapter{Improvements to the Basic Project}

In the previous chapter the focus was on creating a project that can target a dual-core STM32 microcontroller. It was however only a basic "Hello World" type project that can be massively improved upon. In this chapter I will demonstrate the steps that were taken before attempting to build a proper example project.

\section{UART}

Establishing a serial connection between the target MCU and the host PC is usually the most important first step at the start of a new embedded project. While an UART line may not have full debugging capabilities, it allows for quick and relatively easy text based communication with the host. These traces are a very effective tool in determining where a program gets stuck in a loop or panics.

The following code brings up the UART interface that is linked to the ST-Link connector on the board. In this project, this UART interface is used for debugging purposes, so only the rx line is not in use. Rust generates warnings for variables that are not used, this can be avoided by prepending an underscore to the unused variable. The serial line is configured to operate with 19200 baudrate and without flow control.

\begin{lstlisting}[language=C,frame=single,float=!ht,label={lst:uart-bringup},caption={UART Interface Configuration}]
    let tx = gpiod.pd8.into_alternate();
    let _rx = gpiod.pd9.into_alternate();
    let serial = dp
        .USART3
        .serial((tx, _rx), 19200.bps(), ccdr.peripheral.USART3, &ccdr.clocks)
        .unwrap();
    let (mut tx, _rx) = serial.split();
\end{lstlisting}

\section{Debugger}

Using traces on a serial line for detecting errors in the code may not be sufficient in all cases. Moreover the overhead of printing to this peripheral can disrupt the timing of certain parts of the program. In these cases configuring a debugger becomes a necessity.

All STM32 development boards are equipped with an on-board ST-Link debugger. ST-Link connects to the host PC through a USB interface. The ST-Link is able to facilitate debugging through different modes of communication for example single-wire interface module (SWIM), serial wire debugging (SWD) and Joint Test Action Group (JTAG) of which the latter is used most often.

The official IDE provided by STM, STMCubeIDE includes full debugging capabilities, however these tools can only handle C and C++ code. The Rust ecosystem does not currently have a standard tool nor does STM provide debugging tools for other languages. Most often though Rust developers will use openocd, gdb and VSCode.

\subsection{OpenOCD}

Open On-Chip Debugger (OpenOCD) is an open source tool that provides debugging and in-system programming capabilities for embedded devices such as this STM32 microcontroller. It serves as a bridge between the development environment on a host machine and the microcontroller's hardware, facilitating the debugging process.

OpenOCD supports various hardware interfaces, such as JTAG, SWD, and various proprietary interfaces provided by different microcontroller vendors. These interfaces are crucial for establishing a connection between the host machine and the microcontroller, enabling the exchange of debugging information. The software can be configured to the parameters of the target device, such as the CPU architecture, target voltage, and other specific settings. This ensures that the debugger communicates effectively with the microcontroller.

OpenOCD acts as a GDB (GNU Debugger) server, providing a standardized interface for debugging tools. GDB is a full fledged debugger but only its server part is used in this configuration. OpenOCD enables GDB to connect to the target microcontroller, allowing developers to interact with and debug their code. The program also supports in-system programming, allowing users to flash the firmware onto the memory of the microcontroller. This is essential for updating or loading new firmware onto the device during the development and debugging process, however it is also possible to start debugging without flashing in new software, which is ideal for our case as the flashing process for this project is non-trivial due to the two cores.

OpenOCD can be integrated with various Integrated Development Environments and toolchains, providing a seamless debugging experience for developers using different development environments. Being an open-source project, it benefits from a vibrant community that contributes to its development and supports a wide range of hardware platforms. It also allows users to customize and extend its functionality based on their specific debugging requirements.

In the case of this project, the OpenOCD configuration is already provided for the evaluation board. \cite{OpenocdConfigFile}

\subsection{GDB}

The GNU Debugger (GDB) is an open source debugger most often used in linux and embedded development. GDB has a command line interface and can only be used from a terminal so in recent times it is usually replaced by a more modern debugger with a graphical user interface. These debuggers are provided by the chip manufacturer most of the time. However as Rust support for STM32 microcontrollers is community driven, selecting GDB as a debugger is a logical step.

GDB communicates with OpenOCD, which acts as a hardware interface and facilitates communication between the host machine and the Cortex-M microcontroller. OpenOCD establishes the link between GDB and the target device, allowing GDB to exert control over the microcontroller for debugging purposes. The debugger supports ARM architectures, including Cortex-M. It understands the specific characteristics and features of these architectures, allowing developers to debug Rust applications targeting Cortex-M microcontrollers effectively.

GDB has all the features of a modern debugger, only developers need to be familiar with the proper commands. It supports symbolic debugging, enabling developers to use high-level constructs like variable names and function names during the debugging process. This abstraction makes it easier to understand and troubleshoot code behavior at a higher level of abstraction. GDB allows developers to set breakpoints at specific lines or functions in the code. It also supports step-by-step execution, enabling users to navigate through the code, line by line, to identify and diagnose issues. During debugging sessions, GDB provides the capability to inspect and modify variable values in real-time. This feature is crucial for understanding the state of the program and making runtime adjustments as needed. GDB allows developers to evaluate expressions and execute commands during a debugging session. This functionality is valuable for dynamically assessing variables or executing specific code snippets to gain insights into the program's behavior. And most importantly as Rust is an LLVM (Low Level Virtual Machine) compatible language it can fully utilize all of the features of an LLDB (Low Level Debugger) such as the GNU Debugger.

\subsection{VSCode}

The final component of this debugger toolchain setup is Visual Studio Code. While VSCode in and of itself is just a feature rich text editor, with the proper extensions and settings, it is able to act as a full fledged IDE including building, flashing, and remote debugging projects. In the previous section GDB was introduced as a complete debugger but it is still missing a convenient graphical user interface. VSCode is able to provide this interface and handle the GDB commands that need to be executed for the provided utilities.

To configure VSCode correctly, the project must contain at least two additional files placed in the root of the project into a \mycode{.vscode} folder and the Cortex-Debug extension. \cite{CortexDebug} The first file is \mycode{tasks.json}. This file can contains multiple tasks that can be executed by commands in Visual Studio Code. In a project like this, normally two tasks are needed. One to build the project using \mycode{cargo} commands and another one that converts the resulting ELF file into a format that can be loaded onto the microcontroller using an interface supported by OpenOCD. The other file \mycode{launch.json} contains the settings and commands that will actually start and handle the debugging session. This configuration file holds the path to the executable that will be used for the debugging session, the OpenOCD config file and some pre- and postlaunch commands. Using these two files, the debug button in VSCode will build an image with the current source files, load it onto the memory of the microcontroller and start the debugging session. The developer is then able to place breakpoints, stop and start the code as well as step through it line by line.

\section{Cargo Makefile}

In the previous iteration of this project, a Makefile was added around the Cargo project to generate binary and HEX images from the original ELF output. However as this project is already using the Rust ecosystem it may be beneficial to replace the traditional Makefile with a cargo equivalent and reduce the number of external dependencies of this project. The normal cargo buildflow can be supplemented with a \mycode{build.rs} file at the root of the crate. This file will compile and run before the source files are compiled when using the \mycode{cargo build} command. However in our case, the build process contains operations after the build is finished, so another solution is necessary.

The \mycode{cargo-make} \cite{CargoMake} project aims to replace the GNU Makefile with a "rusty" alternative. The tool can be installed using cargo with the command \mycode{cargo install cargo make} and then it can be invoked similarly to GNU make \mycode{cargo make <task>}, where a task is similar to a target in a traditional Makefile. Tasks can be described in a file placed at the crate root titled \mycode{Makefile.toml}. Similarly to how the project can be configured, the build related tasks are to be described in a TOML format. Variables can be set in the \mycode{[env]} section, and tasks can be defined in their own sections under the \mycode{tasks} section. Below is a minimal \mycode{Makefile.toml} file which builds a simple Rust project with the \mycode{cargo make} command.

\begin{lstlisting}[language=C,frame=single,float=!ht,label={lst:cargo-task-example},caption={Cargo Make Task Example}]
    [env]
    OUTPUT_FILENAME = "example"

    [tasks.build]
    clear = true,
    command = "cargo"
    args = [
        "build",
        "--bin", "${OUTPUT_FILENAME}"
    ]

    [tasks.default]
    clear = true,
    dependencies = [ "build" ]
\end{lstlisting}

Some tasks have default defines, for example the \mycode{build} task would just execute the command \mycode{cargo build}, the \mycode{clear = true} line makes sure that all parts of these default definitions are overwritten. The dependencies field array lists all the tasks that need to be executed before the current one is able to run. These dependencies are evaluated in the listed order. For our purposes, cargo-make is a good replacement of a GNU Makefile, but in reality it lacks one important feature that is present in normal make. Currently all dependent tasks are executed before the current one, even if their output is more recent than its requirements. This is a great advantage of Makefiles and a necessity in every build system. The longest part of the build in this project is still the invocation of \mycode{cargo build} command, which only recompiles files with changes, so the solution is still acceptable in our case. However cargo-make is still being developed continually so upgraded dependency handling may be possible in the near future.

\section{Flashing}

So far various ways of flashing our software onto the microcontroller were discussed. \mycode{cargo-flash} seemed to be the most fitting for this project, but using tools provided by ST just seemed more convenient. Setting up a cargo makefile made me reevaluate this aspect of the project and take another look at command line utilities so the deployment process could be automated. The fix to the previously mentioned error in \mycode{st-flash} is still not released at the time of writing, \mycode{cargo-flash} seemed to be the only way forward. It is still only able to flash to one core at a time, however at this stage of the project, where we can be sure that soft resetting the microcontroller will guarantee a steady state, single core flashing will be enough. So three more tasks were added to \mycode{Makefile.toml}, one for flashing each core, and a deploy task that combines these two and building the whole project.

\begin{lstlisting}[language=C,frame=single,float=!ht,label={lst:cargo-make-deploy},caption={Cargo Tasks to Flash the MCU}]
    [tasks.deploy0]
    clear=true
    command = "cargo"
    args = [
        "flash",
        "--elf",
        "${ELF_FILE_0}",
        "--chip",
        "STM32H745ZITx"
    ]

    [tasks.deploy1]
    clear=true
    command = "cargo"
    args = [
        "flash",
        "--elf",
        "${ELF_FILE_1}",
        "--chip",
        "STM32H745ZITx"
    ]

    [tasks.deploy]
    clear = true
    dependencies = ["all", "deploy0", "deploy1"]
\end{lstlisting}

Using this configuration both cores of the microcontroller can be updated after building the current state of the project using a single command: \mycode{cargo make deploy}.

\section{Docker Image}

\subsection{Docker introduction}
The problem of dependency hell \cite{DependencyHell} usually means that libraries used in our code can be dependent on various versions of each other. However the concept can be expanded to compiler tools and toolchains in an embedded project where, for example, the version of the compiler or debugger could be locked by the manufacturer of the microcontroller. This is especially a problem for developers who are working on multiple projects or hardwares as development tools are sometimes installed system-wide. So what can be the solution when multiple projects collide and their dependencies cannot or hardly co-exist on the same system?

Docker is a platform that provides a mean to create and run applications in containers. Containers are portable, lightweight, and self-sufficient units that encapsulate software and its dependencies. This allows us to use the same tools during development and deployment across multiple systems. In embedded software development all the tools used for compiling, flashing, and debugging our code can be included in the container with the libraries these programs depend on. Until Docker became widespread the only way to do this separation was to create virtual machines for each of our projects. Setting up and building docker containers is much faster than the installation of a virtual machine \cite{VMVsDocker}. Our project does not use significant resources and can sit comfortably on the host operating system with the Docker engine providing the isolation.

\subsection{Utilizing Docker in the project}

\subsubsection{Dockerfile}

Using Docker for this project is a two step process. Fist we need to create a Docker image that is able to build, flash, and debug our applications, then we need to configure our editor, VSCode in this case, to work with this container.

To use Docker with a project, we need to place a \mycode{Dockerfile} to the root of it. A \mycode{Dockerfile} is basically a list of instructions on how to build the image. It usually starts with a \mycode{FROM} statement, which signals to the docker engine that our container is derived from another one. In our case it will be derived from the official Rust container. Also the version of this container can be fixed here so the dependencies will not change as newer versions are released to the Docker container store.

\begin{lstlisting}[language=C,frame=single,float=!ht,label={lst:from-rust},caption={Deriving from Rust Container}]
    FROM Rust:1.72
\end{lstlisting}

While having a Rust container is convenient it will not be enough for this project, it needs to be extended with other external programs. Because the Rust container is itself derived from a version of a Debian container, we can use \mycode{apt-get} to install additional programs into our container.

\begin{lstlisting}[language=C,frame=single,float=!ht,label={lst:docker-apt},caption={Installing External Dependencies}]
    RUN apt-get update && apt-get install -y \
        libudev-dev \
        gdb-multiarch \
        picocom \
        openocd \
        stlink-tools \
        xxd \
        binutils-arm-none-eabi \
        srecord

    # Cleaning up to reduce image size
    RUN apt-get autoremove -y
    RUN apt-get clean -y
    RUN apt-get autoclean -y

    RUN ln -s /usr/bin/gdb-multiarch /usr/bin/arm-none-eabi-gdb
\end{lstlisting}

Out of these programs, \mycode{libudev-dev}, \mycode{stlink-tools}, \mycode{openocd}, and \mycode{gdb-multiarch} are used for debugging, while the rest of the programs help during image conversion making binary and HEX files, or in the case of \mycode{srecord} combining two HEX files. A program that can handle serial connections, \mycode{picocom} is also provided as most of the time the easiest way to detect an error is through serial traces.

The rest of the \mycode{apt-get} commands are there to remove any unneeded byproducts that could have entered the system during the previous installation phase. Also, the VSCode debugging interface will search for gdb with deprecated naming (\mycode{gdb-arm-none-eabi-gdb}) so a soft link is created to the correct executable (\mycode{gdb-multiarch}).

Next, the user of this container needs to be created. Later, when the container will be started, some user specific settings and configurations will be transferred into the container from the host machine so we need to create a home directory for the user of the container.

\begin{lstlisting}[language=C,frame=single,float=!ht,label={lst:docker-user},caption={Creating a User for the Container}]
    RUN useradd --create-home --shell /bin/bash rustacean
    USER rustacean
\end{lstlisting}

In the last section of the \mycode{Dockerfile} all the Rust related tools and toolchains are installed using \mycode{cargo} and \mycode{rustup}. As discussed before this project not only needs an arm target to be installed, it also needs the nightly release otherwise some crucial features of the compiler will not work and some crates will break.

\begin{lstlisting}[language=C,frame=single,float=!ht,label={lst:docker-cargo},caption={Installing Rust Specific Tools in the Container}]
    RUN rustup component add llvm-tools-preview
    RUN rustup target add thumbv7em-none-eabihf
    RUN rustup install nightly
    RUN rustup +nightly target add thumbv7em-none-eabihf
    RUN cargo install cargo-binutils --vers 0.3.6
    RUN cargo install cargo-flash
    RUN cargo install microamp-tools --git https://github.com/rtfm-rs/microamp
    RUN cargo install cargo-make
\end{lstlisting}

Even if all these tools can co-exist with other projects on the host, they are conveniently handled by this container, and developers do not need to spend the time of acquiring them one by one. Going through the \mycode{Dockerfile} this way also reveals another advantage. Not only are all the dependencies listed in a clear and concise manner, the way they can be obtained is also described. This makes it easy to understand and recreate the environment which is needed to use a project like this one.

\subsubsection{VSCode DevContainers}
